{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sciencedirect (Elsevier) commands for publications\n",
    "Here it is only about the steps, to parse are necessary so that it can be included in the end."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "url_free1 = 'https://www.sciencedirect.com/science/article/pii/S0140988315002571?via%3Dihub'\n",
    "url_free2 = 'https://www.sciencedirect.com/science/article/pii/S2451929420300851?via%3Dihub'  # url of a open access article\n",
    "\n",
    "url_pay1 = 'https://www.sciencedirect.com/science/article/abs/pii/S104732031830230X?via%3Dihub'  # url of a non subscribed article\n",
    "url_pay2 = 'https://www.sciencedirect.com/science/article/abs/pii/S0306437918300838?via%3Dihub'  # url of a non subscribed article"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from helium import *\n",
    "import cloudscraper\n",
    "from requests_html import HTMLSession\n",
    "import import_ipynb\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Accessing HTML (mutliple ways)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "def get_HTML_selenium(url, os):\n",
    "    \"\"\"\n",
    "    Get HTML from a website using Selenium and ChromeDriver. Methods runs headless per default and has JS activated.\n",
    "    Be aware that this method is quite slow and schould only be used if classic requests method cannot access information thus only use that for dynamic data.\n",
    "    :param url: URL of a website\n",
    "    :param os: Operating system of the user (Windows, Linux, Mac)\n",
    "    :return: HTML with all loaded content\n",
    "    \"\"\"\n",
    "    if os == 'mac':\n",
    "        PATH_MAC = '../driver/chromedriverMAC'\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless=chrome\")\n",
    "    options.add_argument(\"--enable-javascript\")\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\", \"enable-logging\"])\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    driver = webdriver.Chrome(PATH_MAC, options=options)\n",
    "    driver.get(url)\n",
    "    break_time = 5\n",
    "    time.sleep(break_time)\n",
    "\n",
    "    # todo if content owned by WWU add sleep to load more content\n",
    "    html = driver.page_source\n",
    "    driver.close()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    print(\n",
    "        f'Browser closed in {end - start} seconds, including {break_time} seconds of waiting, thus {end - start - break_time} seconds of loading.')\n",
    "    return html\n",
    "\n",
    "\n",
    "def get_page_with_requests(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.5 Safari/605.1.15'}\n",
    "    r = requests.get(url, headers=headers)\n",
    "    print(r.status_code)\n",
    "    assert r.status_code == 200\n",
    "    return r\n",
    "\n",
    "\n",
    "def get_page_with_cloudscraper(url):\n",
    "    scraper = cloudscraper.create_scraper(\n",
    "        browser={\n",
    "            'custom': 'ScraperBot/1.0',\n",
    "        }\n",
    "    )\n",
    "    r = scraper.get(url)\n",
    "    print(r.status_code)\n",
    "    assert r.status_code == 200\n",
    "    return r\n",
    "\n",
    "\n",
    "def get_page_with_requsts_html(url):\n",
    "    s = HTMLSession()\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.5 Safari/605.1.15'}\n",
    "    r = s.get(url, headers=headers)\n",
    "    print(r.status_code)\n",
    "    assert r.status_code == 200\n",
    "    return r\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "def get_bs(url, method='requests'):\n",
    "    try:\n",
    "        if method == 'requests':\n",
    "            r = get_page_with_requests(url)\n",
    "            bs = BeautifulSoup(r.content, 'html.parser')\n",
    "        elif method == 'cloud':\n",
    "            r = get_page_with_cloudscraper(url)\n",
    "            bs = BeautifulSoup(r.content, 'html.parser')\n",
    "        elif method == 'requests_html':\n",
    "            r = get_page_with_requsts_html(url)\n",
    "            bs = BeautifulSoup(r.content, 'html.parser')\n",
    "        elif method == 'selenium':\n",
    "            r = get_HTML_selenium(url, os='mac')\n",
    "            bs = BeautifulSoup(r, 'html.parser')  # selenium already returns html\n",
    "        else:\n",
    "            raise ValueError('Method not known')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}', url)\n",
    "        return None\n",
    "    return bs\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create soups of test objects"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "free1_soup = get_bs(url_free1, method='cloud')\n",
    "free2_soup = get_bs(url_free2, method='cloud')\n",
    "pay1_soup = get_bs(url_pay1, method='cloud')\n",
    "pay2_soup = get_bs(url_pay2, method='cloud')\n",
    "elsevier_soups = [free1_soup, free2_soup, pay1_soup, pay2_soup]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main Fields\n",
    "### Title\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [],
   "source": [
    "def get_title(bs):\n",
    "    \"\"\"\n",
    "    Get title of a publication\n",
    "    :param bs: Bs4 object\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    title = bs.find('span', class_='title-text').text.strip()\n",
    "    return title"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peeling the onion: Analyzing aggregate, national and sectoral energy intensity in the European Union\n",
      "A Structure-Based Platform for Predicting Chemical Reactivity\n",
      "Artistic movement recognition by consensus of boosted SVM based experts\n",
      "Semi-automatic inductive construction of reference process models that represent best practices in public administrations: A method\n"
     ]
    }
   ],
   "source": [
    "check = ['Peeling the onion: Analyzing aggregate, national and sectoral energy intensity in the European Union',\n",
    "         'A Structure-Based Platform for Predicting Chemical Reactivity',\n",
    "         'Artistic movement recognition by consensus of boosted SVM based experts',\n",
    "         'Semi-automatic inductive construction of reference process models that represent best practices in public administrations: A method']\n",
    "\n",
    "for soup, check in zip(elsevier_soups, check):\n",
    "    print(get_title(soup))\n",
    "    assert get_title(soup) == check"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Doi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [],
   "source": [
    "def get_doi(bs, type='doi_number'):\n",
    "    \"\"\"\n",
    "    Gets the doi_number of a publication\n",
    "    :param bs: Bs4 object\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    regex_doi = re.compile(r'http(s?)://doi.org/.*')\n",
    "    doi_link = bs.find('a', class_='doi').text.strip()\n",
    "\n",
    "    if type == 'doi_number':\n",
    "        # control and clean doi\n",
    "        if regex_doi.match(doi_link):\n",
    "            doi = re.sub(r'http(s?)://doi.org/', '', doi_link)\n",
    "            return doi\n",
    "        else:\n",
    "            return None\n",
    "    if type == 'doi_link':\n",
    "        if regex_doi.match(doi_link):\n",
    "            return doi_link\n",
    "        else:\n",
    "            return None\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [
    {
     "data": {
      "text/plain": "'10.1016/j.eneco.2015.09.004'"
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_doi(free1_soup)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Authors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "outputs": [],
   "source": [
    "def get_authors(bs):\n",
    "    \"\"\"\n",
    "    Get authors of a publication\n",
    "    :param bs:\n",
    "    :return: Author names as list\n",
    "    \"\"\"\n",
    "    authors = []\n",
    "    try:\n",
    "        author_boxes = bs.find('div', {'class': 'author-group', 'id': 'author-group'}).find_all('a')\n",
    "\n",
    "        for box in author_boxes:\n",
    "            first_name = box.find('span', class_='text given-name').text.strip()\n",
    "            last_name = box.find('span', class_='text surname').text.strip()\n",
    "            authors.append(f'{first_name} {last_name}')\n",
    "        return authors\n",
    "    except:\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Andreas Löschel', 'Frank Pothen', 'Michael Schymura']\n",
      "['Frederik Sandfort', 'Felix Strieth-Kalthoff', 'Marius Kühnemund', 'Christian Beecks', 'Frank Glorius']\n",
      "['Corneliu Florea', 'Fabian Gieseke']\n",
      "['Hendrik Scholta', 'Marco Niemann', 'Patrick Delfmann', 'Michael Räckers', 'Jörg Becker']\n"
     ]
    }
   ],
   "source": [
    "check = [3, 5, 2, 5]\n",
    "for soup, check in zip(elsevier_soups, check):\n",
    "    print(get_authors(soup))\n",
    "    assert len(get_authors(soup)) == check\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Keywords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "outputs": [],
   "source": [
    "def get_keywords(bs):\n",
    "    \"\"\"\n",
    "    Get list of keywords\n",
    "    :param bs: Received bs of the publication\n",
    "    :return: List of strings\n",
    "    \"\"\"\n",
    "    keywords = []\n",
    "    try:\n",
    "        kwds = bs.find('div', class_='keywords-section').find_all('div', class_='keyword')\n",
    "        for kwd in kwds:\n",
    "            keyword = kwd.text.strip()\n",
    "            keywords.append(keyword)\n",
    "        return keywords\n",
    "    except:\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Environmental and climate economics', 'Energy intensity', 'Index decomposition']\n",
      "['reactivity prediction', 'machine learning', 'molecular structures', 'organic chemistry', 'yield prediction', 'enantioselectivity prediction']\n",
      "['Randomized boosted SVMs', 'Multi-scale topography', 'Painting style recognition', 'Consensus of experts', 'Ensembles']\n",
      "['Process management', 'Process modeling', 'Reference modeling', 'Process model merge', 'E-government', 'Public administration', 'Benchmarking', 'Model querying']\n"
     ]
    }
   ],
   "source": [
    "for soup in elsevier_soups:\n",
    "    print(get_keywords(soup))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Abstract"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [],
   "source": [
    "def get_abstract(bs):\n",
    "    \"\"\"\n",
    "    Get abstract of a publication\n",
    "    :param bs: Received bs of the publication\n",
    "    :return: Abstract : String\n",
    "    \"\"\"\n",
    "    try:\n",
    "        abstract = bs.find('div', class_='abstract author').div.text.strip()\n",
    "        return abstract\n",
    "    except:\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publication: Peeling the onion: Analyzing aggregate, national and sectoral energy intensity in the European Union\n",
      "Authors: Andreas Löschel; Frank Pothen; Michael Schymura; \n",
      "Abstract: \n",
      "One of the most promising ways of meeting climate policy targets is improving energy efficiency, i.e., reducing the amount of scarce and polluting resources needed to produce a given quantity of output. Relying upon the World Input-Output Database (WIOD), we investigate the decline in energy intensity in the EU27 countries between 1995 and 2009. Changes in energy intensity can be attributed to two different drivers: changes in the industrial composition of an economy and changes in its sectoral energy intensities. We conduct a series of index decomposition analyses (IDA) to isolate the effects exerted by these drivers. We then take the findings from the index decomposition analysis and subject them to panel estimations. The objective is to control for factors that may have shaped the evolution of energy intensity in the European Union. We estimate the changes in energy intensity as well as the changes in (energy-relevant) structural change and in the sectoral energy intensities. Therefore, we are able to reveal the channels through which factors such as economic growth, capital intensity, and energy prices affect energy intensity.\n",
      "-------- \n",
      "\n",
      "Publication: A Structure-Based Platform for Predicting Chemical Reactivity\n",
      "Authors: Frederik Sandfort; Felix Strieth-Kalthoff; Marius Kühnemund; Christian Beecks; Frank Glorius; \n",
      "Abstract: \n",
      "Despite their enormous potential, machine learning methods have only found limited application in predicting reaction outcomes, because current models are often highly complex and, most importantly, are not transferable to different problem sets. Here, we present a structure-based machine learning platform for diverse applications in organic chemistry. Therefore, an input based on multiple fingerprint features (MFFs) as a versatile molecular representation was developed that was shown to be applicable over a range of diverse problem sets. First, molecular properties across a diverse array of molecules could be predicted accurately. Next, reaction outcomes such as stereoselectivities and yields were predicted for experimental datasets that were previously evaluated using (complex) problem-oriented descriptor models. As a final application, a systematic high-throughput dataset was investigated as a “real-world problem,” and good correlation was observed when using the structure-based model.\n",
      "-------- \n",
      "\n",
      "Publication: Artistic movement recognition by consensus of boosted SVM based experts\n",
      "Authors: Corneliu Florea; Fabian Gieseke; \n",
      "Abstract: \n",
      "In this work we aim to automatically recognize the artistic movement from a digitized image of a painting. Our approach uses a new system that resorts to descriptions induced by color structure histograms and by novel topographical features for texture assessment. The topographical descriptors accumulate information from the first and second local derivatives within four layers of finer representations. The classification is performed by two layers of ensembles. The first is an adapted boosted ensemble of support vector machines, which introduces further randomization over feature categories as a regularization. The training of the ensemble yields individual experts by isolating initially misclassified images and by correcting them in further stages of the process. The solution improves the performance by a second layer build upon the consensus of multiple local experts that analyze different parts of the images. The resulting performance compares favorably with classical solutions and manages to match the ones of modern deep learning frameworks.\n",
      "-------- \n",
      "\n",
      "Publication: Semi-automatic inductive construction of reference process models that represent best practices in public administrations: A method\n",
      "Authors: Hendrik Scholta; Marco Niemann; Patrick Delfmann; Michael Räckers; Jörg Becker; \n",
      "Abstract: \n",
      "Business process management often uses reference models to improve processes or as starting point when creating individual process models. The current academic literature offers primarily deductive methods with which to develop these reference models, although some methods develop reference models inductively from a set of individual process models, focusing on deriving and representing common practices. However, there is no inductive method with which to detect best practices and represent them in a reference model. This paper addresses this research gap by proposing a method by which to develop reference process models that represent best practices in public administrations semi-automatically and inductively. The method uses a merged model that retains the structure of the source models while detecting their common parts. It identifies best practices using query constructs and ranking criteria to group the source models’ elements and to evaluate these groups. We provide a conceptualization of the method and demonstrate its functionality using an artificial example. We describe our implementation of the method in a software prototype and report on its evaluation in a workshop with domain and method experts who applied the method to real-world process models.\n",
      "-------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for soup in elsevier_soups:\n",
    "    print(f'Publication: {get_title(soup)}')\n",
    "    a = ''\n",
    "    for x in get_authors(soup):\n",
    "        a += (x + '; ')\n",
    "    print(f'Authors: {a}')\n",
    "    print('Abstract: ')\n",
    "    print(get_abstract(soup))\n",
    "    print('-------- \\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Journal fields\n",
    "### Journal name"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "outputs": [],
   "source": [
    "def get_journal_name(bs):\n",
    "    \"\"\"\n",
    "    Get the journal name where the paper has been published\n",
    "    :param bs: Received bs of the publication\n",
    "    :return: String\n",
    "    \"\"\"\n",
    "    # Some Journals have their name as text and logo, other have their name as text only and a dedicated logo\n",
    "    # We have to differentiate between both cases\n",
    "    journal_bar = bs.find('div', {'id': 'publication'})\n",
    "    if journal_bar.attrs['class'] == ['Publication', 'wordmark-layout']:\n",
    "        journal_name = journal_bar.find('h2', class_=lambda c: 'publication-title-link' in c).text.strip()\n",
    "    else:\n",
    "        journal_name = journal_bar.find('a', class_='publication-title-link').text.strip()\n",
    "    return journal_name"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy Economics\n",
      "Chem\n",
      "Journal of Visual Communication and Image Representation\n",
      "Information Systems\n"
     ]
    }
   ],
   "source": [
    "check = ['Energy Economics', 'Chem', 'Journal of Visual Communication and Image Representation', 'Information Systems']\n",
    "\n",
    "for soup, check in zip(elsevier_soups, check):\n",
    "    print(get_journal_name(soup))\n",
    "    assert get_journal_name(soup) == check"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Journal information (Volume, Publication Month/Year, Pages)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "outputs": [],
   "source": [
    "# Information are parsed from an an unstructured div-element and thus extracted in the same method\n",
    "def get_journal_information(bs):\n",
    "    \"\"\"\n",
    "    Returns the journal information of a publication\n",
    "    :param bs: bs4 object\n",
    "    :return: Dictionary with journal information: Volume, Release, Start page, End page\n",
    "    \"\"\"\n",
    "    try:\n",
    "        journal_info = bs.find('div', {'class': 'Publication', 'id': 'publication'}).find('div',\n",
    "                                                                                          class_='text-xs')\n",
    "        # dummy comment to identify type\n",
    "        comment_markup = \"<b><!--I am an comment--></b>\"\n",
    "        _x = BeautifulSoup(comment_markup, \"html.parser\")\n",
    "        _comment = _x.b.string\n",
    "\n",
    "        counter = 0  # count HTML comments\n",
    "        result = ['Volume', 'Year', 'PageRange']  # create list of results\n",
    "        # iterate over sub-content of textbox\n",
    "        for x in journal_info.contents:\n",
    "            if x.text.strip() == ',':  # skip when comma is found\n",
    "                continue\n",
    "            # increment counter if comment is found\n",
    "            if isinstance(x, type(_comment)):\n",
    "                counter += 1\n",
    "                continue\n",
    "            # add text to result list if no special case  (comment, comma\n",
    "            result[counter] = x.text.strip()\n",
    "\n",
    "        # clean page range\n",
    "        result[2] = result[2].removeprefix(', Pages ')\n",
    "        volume, release, page_range = result\n",
    "        # Split by hyphen to get start and end_page\n",
    "        start_page = page_range.split('-')[0]\n",
    "        end_page = page_range.split('-')[1]\n",
    "\n",
    "        return {\n",
    "            'volume': volume, 'release': release, 'start_page': start_page, 'end_page': end_page\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_volume(bs):\n",
    "    \"\"\"\n",
    "    Returns the information about the volume of a jourmal in which the publication has been published\n",
    "    :param bs: bs4 object\n",
    "    :return: String\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return get_journal_information(bs)['volume']\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_release(bs):\n",
    "    \"\"\"\n",
    "    Returns the information about the release of a jourmal in which the publication has been published\n",
    "    :param bs: bs4 object\n",
    "    :return: String\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return get_journal_information(bs)['release']\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_start_page(bs):\n",
    "    \"\"\"\n",
    "    Returns the start page of a publication in a journal\n",
    "    :param bs: bs4 object\n",
    "    :return: String\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return get_journal_information(bs)['start_page']\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_end_page(bs):\n",
    "    \"\"\"\n",
    "    Returns the end page of a publication in a journal\n",
    "    :param bs: bs4 object\n",
    "    :return: String\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return get_journal_information(bs)['end_page']\n",
    "    except:\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peeling the onion: Analyzing aggregate, national and sectoral energy intensity in the European Union\n",
      "{'volume': 'Volume 52, Supplement 1', 'release': 'December 2015', 'start_page': 'S63', 'end_page': 'S75'}\n",
      "Volume: Volume 52, Supplement 1\n",
      "Release: December 2015\n",
      "Start page: S63\n",
      "End page: S75\n",
      "-------- \n",
      "\n",
      "A Structure-Based Platform for Predicting Chemical Reactivity\n",
      "{'volume': 'Volume 6, Issue 6', 'release': '11 June 2020', 'start_page': '1379', 'end_page': '1390'}\n",
      "Volume: Volume 6, Issue 6\n",
      "Release: 11 June 2020\n",
      "Start page: 1379\n",
      "End page: 1390\n",
      "-------- \n",
      "\n",
      "Artistic movement recognition by consensus of boosted SVM based experts\n",
      "{'volume': 'Volume 56', 'release': 'October 2018', 'start_page': '220', 'end_page': '233'}\n",
      "Volume: Volume 56\n",
      "Release: October 2018\n",
      "Start page: 220\n",
      "End page: 233\n",
      "-------- \n",
      "\n",
      "Semi-automatic inductive construction of reference process models that represent best practices in public administrations: A method\n",
      "{'volume': 'Volume 84', 'release': 'September 2019', 'start_page': '63', 'end_page': '87'}\n",
      "Volume: Volume 84\n",
      "Release: September 2019\n",
      "Start page: 63\n",
      "End page: 87\n",
      "-------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Small test\n",
    "for soup in elsevier_soups:\n",
    "    print(get_title(soup))\n",
    "    print(get_journal_information(soup))\n",
    "    print(f'Volume: {get_volume(soup)}')\n",
    "    print(f'Release: {get_release(soup)}')\n",
    "    print(f'Start page: {get_start_page(soup)}')\n",
    "    print(f'End page: {get_end_page(soup)}')\n",
    "    print('-------- \\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}